{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/f/fb/SETU_Ireland_logo.png\" alt=\"SETU\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palmer Archipelago Penguin Data Set\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Author: John Paul Lee\n",
    "* Github: JPLee01\n",
    "* Email: C00244423@itcarlow.ie\n",
    "* Created: 08-11-2022, Last update: 09-11-2022\n",
    "* Research Method for Engineering: Data Analysis and Visualisation 30%.\n",
    "***\n",
    "**Lecturer:** Dr. Edmond Tobin\n",
    "\n",
    "* This Jupyter Notebook has been created to offer a report on the Palmer Archipelago Penguin Data Set. This report will provide an overview of the data set, examine the data set in terms of variables, execute normality, homogeneity and post hoc tests. Provide plots and visualisations as necessary.\n",
    "\n",
    "***\n",
    "As part of the assignment this report will deal with four main tasks:\n",
    "\n",
    "1. An analysis of the 4 dependent variables within the Data Set. \n",
    "2. Execution of normality and homogeneity tests.\n",
    "3. Execution of post hoc tests.\n",
    "4. Creation of plots and visualisations as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "[Penguin Data Set](#penguin-data-set)\n",
    "\n",
    "[1. Introduction](#1-introduction)\n",
    "\n",
    "[2. Penguin Data Set](#2-penguin-data-set)\n",
    "\n",
    "  [2.1 Background](#2.1-Background)\n",
    "\n",
    "[3. Problem Statement](#3-Problem-Statement)\n",
    "\n",
    "[4. Previous Case Studies of the Data Set](#4-Previous-Case-Studies-of-the-Data-Set)\n",
    "\n",
    "[5. Issues and Inconsistencies with Data Set](#5-Issues-and-Inconsistencies-with-Data-Set)\n",
    "\n",
    "[6. Libraries](#6-Libraries)\n",
    "\n",
    "[7. Analysis of Data Set and Results Explained](#7-Analysis-of-Data-Set-and-Results-Explained)\n",
    "\n",
    "  [7.1 Initial Analysis](#7.1-Initial-Analysis)\n",
    "\n",
    "  [7.2 Normality Analysis](#7.2-Normality-Analysis)\n",
    "\n",
    "  [7.3 Homogeneity Analysis](#7.3-Homogeneity-Analysis)\n",
    "\n",
    "  [7.4 Post Hoc Analysis](#7.4-Post-Hoc-Analysis)\n",
    "\n",
    "[8. Summary, Conclusion and Future Possibilities for the Data Set](#8-Summary,-Conclusion-and-Future-Possibilities-for-the-Data-Set)\n",
    "\n",
    "[9. References](#9-References)\n",
    "\n",
    "[10. Bibliography](#10-Bibliography)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "This analysis of Penguin Data Set has been carried out as an assignment of the Research Methods Module. The aim of this assignment is to help the student gain a practical experience in data handling, including data types and structures handling, data splicing, plots generation and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Palmer Archipelago Penguin Iris Data Set\n",
    "------------------------------------------------------------------------------------------------\n",
    "### 2.1 Background\n",
    "The data analysed in this report is the Palmer Archipelago Penguin Data Set (Gorman et al., 2014).The Penguin Data Set collected from 2007 to 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network (Hill et al., 2020). Body size measurements were collected for three species of Pygoscelis penguins that breed on islands throughout the Palmer Archipelago, Antarctica (Gorman et al., 2021). The three species of Pygoscelis penguins can be seen in the image below:\n",
    "\n",
    "![Penguin Species](https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/gorman-penguins.jpg)\n",
    "\n",
    "Three hundred and fourty four observations were collected and analysed. Within each observation, Gorman studied five distinct characteristics:\n",
    "\n",
    "1.  Bill Length (Mm)\n",
    "2.  Bill Depth (Mm)\n",
    "3.  Flipper Length (Mm)\n",
    "4.  Body Mass (G)\n",
    "5.  Sex\n",
    "\n",
    "These observations were taken over the following three islands of the Palmer Archipelago:\n",
    "1. Biscoe \n",
    "2. Dream\n",
    "3. Torgersen\n",
    "\n",
    "It should be noted that the year these observations took place is also included in the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3 Problem Statement\n",
    "------------------------------------------------------------------------------------------------\n",
    "As part of the assignment, the student was given a set of instructions. Within the instructions a problem statement was printed. It stated that this assignment will involve an analysis of the Palmer Archipelago Penguin Data Set. Python code will be utilised for this analysis and the student will give explanations of the python code and discuss any observations. The problem statement also explained it was required for the project to:\n",
    "\n",
    "* Produce a statistical analysis report of the data and present a series of graphs of the data that would be in a form suitable for publication in a journal paper or other formal report.\n",
    "* Analyse all 4 dependent variables. \n",
    "* Perform normality and homogeneity tests, and if required, post hoc tests.\n",
    "* The report should provide a discussion on the significance of the data and data set.\n",
    "* The student can use any statistical software that they are familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4 Previous Case Studies of the Data Set\n",
    "------------------------------------------------------------------------------------------------\n",
    "Through an online search it can be seen that a number of previous iterations of the analysis of the data have taken place. These include programs written in Python as well as other computing languages. The works of Tsai (2021) and Tirendaz Academy (2022) who demonstrate the use of Pandas, Numpy, Matplotlib and Seaborn for management of the Palmer Archipelago Penguin Data Set were used as inspiration by the author in the construction of the report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5 Issues within the Data Set\n",
    "------------------------------------------------------------------------------------------------\n",
    "It should be noted, that there are a number of issues within the data set sourced provided to the author and the data set presented by Gorman (2014) et al. These issues are as follows:\n",
    "\n",
    "1.  4th Sample:  Null values in every column except species, island and year.\n",
    "2.  9th Sample:  Null sex value.\n",
    "3.  10th Sample: Null sex value.\n",
    "4.  11th Sample: Null sex value.\n",
    "5.  12th Sample: Null sex value.\n",
    "6.  48th Sample: Null sex value.\n",
    "7.  179th Sample: Null sex value.\n",
    "8.  219th Sample: Null sex value.\n",
    "9.  257th Sample: Null sex value.\n",
    "10. 269th Sample: Null sex value.\n",
    "11. 272nd Sample:  Null values in every column except species, island and year.\n",
    "\n",
    "These issues will be handled by the author later on in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##  6 Libaries\n",
    "------------------------------------------------------------------------------------------------\n",
    "The following Python libraries were used in the writing of the programs code and are required to successfully run the notebook:\n",
    "* [Numpy](https://www.numpy.org/) - Used for synthesisation of the data and mathematical functions.\n",
    "* [Pandas](https://pandas.pydata.org/) - Used for import, management, data manipulation and analysis.\n",
    "* [Matplotlib.pyplot](https://matplotlib.org/tutorials/introductory/pyplot.html) - Used for the manipulation of elements and the creation of certain plots graphs, plots and charts.\n",
    "* [Seaborn](https://seaborn.pydata.org/) - Used for the creation and manipulation of all plots created. (Seaborn allows for the extetion of the functionality of Matplotlib).\n",
    "* [Scikit-Learn](https://scikit-learn.org/stable/) - A machine learning library for the Python programming language. The use of this library is explored within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Analysis of Data Set and Results Explained\n",
    "------------------------------------------------------------------------------------------------\n",
    "This section will describe the Python programs and subsequent code which was created to analyse the data set as well as a discussion on the results these programs yielded.\n",
    "\n",
    "For clarity comments within the code have been added by the author to offer the reader an understanding of the specifics of the Python programs created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Initial Analysis\n",
    "This section will be concerned with the loading of the necessary libraries, the importing the data set and creation of a DataFrame as well as an initial analysis of the contents of the data set.\n",
    "\n",
    "#### Importing the Libraries\n",
    "As seen above a munber of libraries are required to successfully write and run the programs code. For the notebook the following libraries were imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation of the necessary libraries\n",
    "\n",
    "#Import Numpty for Analysis of the data \n",
    "import numpy as np\n",
    "\n",
    "#Import Pandas for Data Management \n",
    "import pandas as pd \n",
    "\n",
    "#Import matplotlib.pyplot and seaborn for Visualisation of the data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Import Scikit-Learn for Machine Learning of the data\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point to note ss we will be displaying Plots in this Jupyter Notebook we will implement the inline magic command to allow the Plots to be rendered inline within the Notebook (Rishabh, 2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inline Magic command implemented to ensure that the Plots are rendered inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure uniformity throughout the Juypter Notebook with regard to Seaborn Plots display, the style and palette fuction will be set.\n",
    "\n",
    "The style function will be set to darkgrid. This will allow for optimal measurements of Plots, as the darkened background with the built in grid lines will be best displayed against the white background of the Juypter Notebook.(Geek for Geeks, 2021). The palette fuction will be set to bright, as it will allow for clear distinction of multiple outputs within one Plot (Geek for Geeks, 2021). Finally, in order to ensure uniformity throughout the notebook the plots size will be set using the rcParams function (Dou, 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting of Seaborn dispays to enure uniformity throughout the Juypter Notebook\n",
    "\n",
    "#Darkplot style selected to allow for optimal measurments of Plots\n",
    "sns.set_style(\"darkgrid\")\n",
    "#Bright colour palette selected to allow for clear distinction of multiple outputs within one Plot \n",
    "sns.set_palette(\"bright\")\n",
    "\n",
    "# set plot style\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Increase the size of the output plots\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Data Set and Creation of a DataFrame\n",
    "The data set is imported from the Csv file through the use of the pandas.read function. Once imported this data set is used in the creation of a DataFrame to allow for greater ease of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the data\n",
    "f = pd.read_csv(\"Penguin.csv\")\n",
    "\n",
    "#Create a DataFrame from the Csv file to allow for easier analysis\n",
    "df = pd.DataFrame(f)\n",
    "\n",
    "#Copy of the original data set kept in case required\n",
    "dfc = pd.DataFrame(f)\n",
    "\n",
    "#Display the shape of the DataFrame\n",
    "(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the shape the the data set contains three hundred and fourty four observations with eight variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of the Unique Values in the Data Set\n",
    "To create a list of the unique values in the categorical columns the following code is executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Species present in this dataset are:\n",
      "> Adelie\n",
      "> Gentoo\n",
      "> Chinstrap\n",
      "\n",
      "The Islands present in this dataset are:\n",
      "> Torgersen\n",
      "> Biscoe\n",
      "> Dream\n",
      "\n",
      "The Sex types present in this dataset are:\n",
      "> male\n",
      "> female\n",
      "> nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unique Values in the Species Column\n",
    "Specieslist = df[\"species\"].unique()\n",
    "print (\"The Species present in this dataset are:\")\n",
    "for i in Specieslist:\n",
    "    print('>', i)\n",
    "print()\n",
    "    \n",
    "#Unique Values in the Island Column\n",
    "Islandlist = df[\"island\"].unique()\n",
    "print (\"The Islands present in this dataset are:\")\n",
    "for i in Islandlist:\n",
    "    print('>', i)\n",
    "print()\n",
    "\n",
    "#Unique Values in the Sex Column\n",
    "Genderlist = df[\"sex\"].unique()\n",
    "print (\"The Sex types present in this dataset are:\")\n",
    "for i in Genderlist:\n",
    "    print('>', i)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the results it appears a third sex type value is present. This lends itself to the assumption that there may be some inconsistencies within the data set. As a result, an exploration of this will now take place (Data to Fish, 2021). Firstly the Pandas.DataFrame.head command will be implemented to display the first 5 rows of data to see if any Nan (not a number/null) values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The follwoing are the first 5 rows of data within the data set:\n",
      "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
      "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
      "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
      "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
      "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
      "\n",
      "   body_mass_g     sex  year  \n",
      "0       3750.0    male  2007  \n",
      "1       3800.0  female  2007  \n",
      "2       3250.0  female  2007  \n",
      "3          NaN     NaN  2007  \n",
      "4       3450.0  female  2007   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the first 5 rows of data\n",
    "print(\"The follwoing are the first 5 rows of data within the data set:\")\n",
    "print(df.head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen row 3 (4th observation but to the rows beginning at 0) null values in every column except species, island and year. As a result, code will be implemented which will highlight all rows which have a NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "3    Adelie  Torgersen             NaN            NaN                NaN   \n",
      "8    Adelie  Torgersen            34.1           18.1              193.0   \n",
      "9    Adelie  Torgersen            42.0           20.2              190.0   \n",
      "10   Adelie  Torgersen            37.8           17.1              186.0   \n",
      "11   Adelie  Torgersen            37.8           17.3              180.0   \n",
      "47   Adelie      Dream            37.5           18.9              179.0   \n",
      "178  Gentoo     Biscoe            44.5           14.3              216.0   \n",
      "218  Gentoo     Biscoe            46.2           14.4              214.0   \n",
      "256  Gentoo     Biscoe            47.3           13.8              216.0   \n",
      "268  Gentoo     Biscoe            44.5           15.7              217.0   \n",
      "271  Gentoo     Biscoe             NaN            NaN                NaN   \n",
      "\n",
      "     body_mass_g  sex  year  \n",
      "3            NaN  NaN  2007  \n",
      "8         3475.0  NaN  2007  \n",
      "9         4250.0  NaN  2007  \n",
      "10        3300.0  NaN  2007  \n",
      "11        3700.0  NaN  2007  \n",
      "47        2975.0  NaN  2007  \n",
      "178       4100.0  NaN  2007  \n",
      "218       4650.0  NaN  2008  \n",
      "256       4725.0  NaN  2009  \n",
      "268       4875.0  NaN  2009  \n",
      "271          NaN  NaN  2009  \n"
     ]
    }
   ],
   "source": [
    "#Highlight all rows which have a NaN value\n",
    "nan_values = df[df.isnull().any(axis=1)]\n",
    "\n",
    "#Print the results\n",
    "print (nan_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can see that eleven observtions have Nan (not a number/null) values. These range from a single entry e.g. sex, in an observation, to every entries except species, island and year. This also reinforces the issues highlighed in section 5 of the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treatment of Null Vlaues\n",
    "When analysing a data set it is important to deal with null values. This is becasue if left untreated null values can adversely affect the accuracy of resluts/hypotheses as well as potentially building a biased machine learning algorithms (Kumar, 2019). When data wrangling (removing errors and combining data sets to make them more accessible and easier to analyze (Simplilearn, 2022)), two primary methods can be implemented; the imputation or the removal of data (Swalin, 2018). \n",
    "\n",
    "The imputation of data is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset i.e. developing a reasonable guess for the missing data (Singhal, 2021). This technique ia most useful when the percentage of missing data is low. However if, the portion of missing data is too high, the results lack natural variation that could result in an effective model (MastersInDataScience.org, 2022). Examples of imputation techniques include; Mean imputation (replacing the missing value withe the mean (Schork, 2019) and Cold deck imputation (replacing the missing value with a value collected from a similiar study (Maity, 2020). \n",
    "\n",
    "The removal of data simply mean the removal of the observations with the missing data from the data set (Madaan, 2022). While it can be argued the removal of data versus the imputation of data reduces bias, removing data may not be the best option if there are not enough observations to result in a reliable analysis (Gawali, 2021). \n",
    "\n",
    "It is the author opinion that for the purpose of this assignment the imputation of data is most suitable technique to use. This is due to the fact that the overall presentage of missing data is less than 1% (19 missing values out of a total of 2752) and machine learning techniques offer an effective method to impute missing values. The author will implement the Sklearn KNN Imputer module to predict the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Sklearn KNN Imputer on Null Vlaues\n",
    "KNN Imputer is a scikit-learn class used to fill out or predict the missing values in a dataset (Geek for Geeks, 2020).The KNN Imputer class uses the k-Nearest Neighbors approach for calculating the missing values (Brownlee, 2020). That is to say ‘k’ samples in the dataset that are similar or close to the missing data are identified, the mean value of these ‘k’ samples are used to predict the missing values (Chowdhury, 2020). \n",
    "\n",
    "It should be noted that the KNN Imputer does not work with object datatypes e.g. string, array, class. As a result all the object data will need to be converted to an integer type via sklearn’s LabelEncoder (The Data Detective, 2019).\n",
    "\n",
    "The implement the Sklearn KNN Imputer module can be seen in the code below.\n",
    "\n",
    "Please note that when implementing the Sklearn KNN Imputer module on the DataFrame copy a Python Warning appears. This particular warning \"SettingWithCopyWarning\" states that - A value is trying to be set on a copy of a slice from a DataFrame. Explained in great details in a DataQuest blog (2017) that warning essentially informs that user that they may be modifying a copy of a DataFrame but not the original DataFrame (this can be seen in the image below). When implementing the Sklearn KNN Imputer module it is the authors intention to modify to copied DataFrame (dfc) and leave the original DataFrame (df) to allow for comparassion. As a result, for aesthetic purposes the author will implement the Python warnings ignore function (Stack Overflow, 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SettingWithCopyWarning](https://www.dataquest.io/wp-content/uploads/2019/01/modifying.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import the warnings ignore function for aesthetic purposes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Import KNNImputer and LabelEncoder from the sklearn library\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Encode and Impute the data\n",
    "encode = LabelEncoder()\n",
    "impute = KNNImputer()\n",
    "for i in df.select_dtypes(include='object').columns:\n",
    "    df[i][df[i].notnull()] = encode.fit_transform(dfc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas.DataFrame.head command will be implemented will now be implemented again to observed how the data has been encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  species island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0       0      2            39.1           18.7              181.0   \n",
      "1       0      2            39.5           17.4              186.0   \n",
      "2       0      2            40.3           18.0              195.0   \n",
      "3       0      2             NaN            NaN                NaN   \n",
      "4       0      2            36.7           19.3              193.0   \n",
      "\n",
      "   body_mass_g  sex  year  \n",
      "0       3750.0    1  2007  \n",
      "1       3800.0    0  2007  \n",
      "2       3250.0    0  2007  \n",
      "3          NaN  NaN  2007  \n",
      "4       3450.0    0  2007  \n"
     ]
    }
   ],
   "source": [
    "#Use of Pandas.DataFrame.head command again to see how data is encoded\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can be seen in the above output the results in the columns are not scaled properly. This can potentially lead to issues in terms of data analysis. As a result the Sklearn MinMaxScaler module will be implemented to to normalize the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.269091</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.298182</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0      0.0     1.0        0.254545       0.666667           0.152542   \n",
       "1      0.0     1.0        0.269091       0.511905           0.237288   \n",
       "2      0.0     1.0        0.298182       0.583333           0.389831   \n",
       "3      0.0     1.0             NaN            NaN                NaN   \n",
       "4      0.0     1.0        0.167273       0.738095           0.355932   \n",
       "\n",
       "   body_mass_g  sex  year  \n",
       "0     0.291667  1.0   0.0  \n",
       "1     0.305556  0.0   0.0  \n",
       "2     0.152778  0.0   0.0  \n",
       "3          NaN  NaN   0.0  \n",
       "4     0.208333  0.0   0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sca = MinMaxScaler()\n",
    "df_sca = pd.DataFrame(sca.fit_transform(df),columns=dfc.columns)\n",
    "df_sca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species              0\n",
       "island               0\n",
       "bill_length_mm       0\n",
       "bill_depth_mm        0\n",
       "flipper_length_mm    0\n",
       "body_mass_g          0\n",
       "sex                  0\n",
       "year                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(impute.fit_transform(df_sca), columns=dfc.columns)\n",
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.DataFrame(sca.inverse_transform(new_df),columns=df.columns)\n",
    "\n",
    "dt['sex'] = dt['sex'].round()\n",
    "\n",
    "dt['sex'] = dt['sex'].map({0:'Female', 1:'Male'})\n",
    "\n",
    "dt['species'] = dt['species'].map({0:'Adeile',1:'Chinstrap',2:'Gentoo'})\n",
    "\n",
    "dt['island'] = dt['island'].map({0:'Biscoe',1:'Dream',2:'Torgersen'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adeile</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.70</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adeile</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.40</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adeile</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.00</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adeile</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>38.3</td>\n",
       "      <td>19.46</td>\n",
       "      <td>191.2</td>\n",
       "      <td>3960.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adeile</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.30</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>2007.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adeile  Torgersen            39.1          18.70              181.0   \n",
       "1  Adeile  Torgersen            39.5          17.40              186.0   \n",
       "2  Adeile  Torgersen            40.3          18.00              195.0   \n",
       "3  Adeile  Torgersen            38.3          19.46              191.2   \n",
       "4  Adeile  Torgersen            36.7          19.30              193.0   \n",
       "\n",
       "   body_mass_g     sex    year  \n",
       "0       3750.0    Male  2007.0  \n",
       "1       3800.0  Female  2007.0  \n",
       "2       3250.0  Female  2007.0  \n",
       "3       3960.0  Female  2007.0  \n",
       "4       3450.0  Female  2007.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0       0      2            39.1           18.7              181.0   \n",
       "1       0      2            39.5           17.4              186.0   \n",
       "2       0      2            40.3           18.0              195.0   \n",
       "3       0      2             NaN            NaN                NaN   \n",
       "4       0      2            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g  sex  year  \n",
       "0       3750.0    1  2007  \n",
       "1       3800.0    0  2007  \n",
       "2       3250.0    0  2007  \n",
       "3          NaN  NaN  2007  \n",
       "4       3450.0    0  2007  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  9 References\n",
    "------------------------------------------------------------------------------------------------\n",
    "* Brownlee, J. (2020). *kNN Imputation for Missing Values in Machine Learning*, Machine Learning Mastery [online], 24 June, availalbe: https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/ [Accessed 09 November 2022].\n",
    "* Chowdhury, K. (2020). *KNNImputer: A robust way to impute missing values (using Scikit-Learn)*, Analytics Vidhya [online], 14 July, availalbe: https://www.analyticsvidhya.com/blog/2020/07/knnimputer-a-robust-way-to-impute-missing-values-using-scikit-learn/#:~:text=The%20idea%20in%20kNN%20methods,neighbors%20found%20in%20the%20dataset. [Accessed 09 Novmeber 2022].\n",
    "* Data to Fish. (2021). *Select all Rows with NaN Values in Pandas DataFrame*, Data to Fish [online], 17 July, availalbe: https://datatofish.com/rows-with-nan-pandas-dataframe/ [Accessed 09 November 2022].\n",
    "* DataQuest (2017). *SettingwithCopyWarning: How to Fix This Warning in Pandas*, DataQuest [online], 05 July, availalbe: https://www.dataquest.io/blog/settingwithcopywarning/ [Accessed 09 Novmeber 2022].\n",
    "* Dou, S. (2020). *How To Create And Use Custom Matplotlib Style Sheet*, Towards Data Science [online], 17 October, available: https://towardsdatascience.com/how-to-create-and-use-custom-matplotlib-style-sheet-9393f498063 [Accessed 09 November 2022].\n",
    "* Gawali, S. (2021). *How to Deal with Missing Data using Python*, Analytics Vidhya [online], 09 October, available: https://www.analyticsvidhya.com/blog/2021/10/how-to-deal-with-missing-data-using-python/ [Accessed 09 November 2022].\n",
    "* Geek for Geeks. (2020). *Python | Imputation using the KNNimputer()*, Geek for Geeks [online], 05 September, availalbe: https://www.geeksforgeeks.org/python-imputation-using-the-knnimputer/#:~:text=KNNimputer%20is%20a%20scikit%2Dlearn,with%20mean%20or%20the%20median. [Accessed 09 November 2022].\n",
    "* Geek for Geeks. (2021). *Seaborn | Style And Color*, Geek for Geeks [online], 29 Janurary, available: https://www.geeksforgeeks.org/seaborn-style-and-color/ [Accessed 09 November 2022].\n",
    "* Gorman, K.B. and at al. (2014). *Ecological sexual dimorphism and environmental variability within a community of Antarctic penguins (genus Pygoscelis)*. PloS one, 9(3), pp. 81-90.\n",
    "* Gorman K.B. and et al. (2021). *Advancing the Sea Ice Hypothesis: Trophic Interactions Among Breeding Pygoscelis Penguins With Divergent Population Trends Throughout the Western Antarctic Peninsula*, Frontiers in Marine Science, 8(1), pp. 1-8.\n",
    "* Hill, A. and et al., (2020). *Release the penguins*, R Studio Education [online], 27 July, available: https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/#meet-the-penguins [Accessed 08 November 2022]. \n",
    "* Kumar, N. (2019). *Data Wrangling: Removing Null Values from Dataset in Python using Pandas Library*, The Professionals Point [online], 15 March, available: http://theprofessionalspoint.blogspot.com/2019/03/data-wrangling-removing-null-values.html#:~:text=Removing%20null%20values%20from%20the,learning%20algorithm%20to%20that%20dataset. [Accessed 09 November 2022].\n",
    "* Madaan, M. (2022). *Handling missing values: Beginners Tutorial*, Naukri Learning [online], 04 April, available: https://www.naukri.com/learning/articles/handling-missing-values-beginners-tutorial/ [Acced 09 November 2022]. \n",
    "* Maity, A. (2020). *Blog On The Techniques of Data Imputation By Amarjit Maity*, Linkenin [online], 15 October, availalbe: https://www.linkedin.com/pulse/techniques-data-imputation-amarjit-maity [Accessed 09 November 2022].\n",
    "* MastersInDataScience.org. (2022). *How to Deal with Missing Data*, MastersInDataScience.org [online], 01 January, availalbe: https://www.mastersindatascience.org/learning/how-to-deal-with-missing-data/#:~:text=When%20dealing%20with%20missing%20data,of%20missing%20data%20is%20low. [Accessed 09 November 2022].\n",
    "* Rishabh. (2017). *Purpose of \"%matplotlib inline\"*, Stack Overflow [online], 26 March, available: https://stackoverflow.com/questions/43027980/purpose-of-matplotlib-inline/43028034 [Accessed 09 November 2022]. \n",
    "* Simplilearn. (2022). *What Is Data Wrangling? Benefits, Tools, Examples and Skills*, Simplilearn [online], 12 July, available: https://www.simplilearn.com/data-wrangling-article#:~:text=Data%20wrangling%20is%20the%20process%20of%20removing%20errors%20and%20combining,analysis%20is%20becoming%20increasingly%20necessary [Accessed 09 November 2022].\n",
    "* Singhal, S. (2021). *Defining, Analysing, and Implementing Imputation Techniques*, Analytics Vidhya [online], 21 June, available: https://www.analyticsvidhya.com/blog/2021/06/defining-analysing-and-implementing-imputation-techniques/#:~:text=What%20is%20Imputation%3F,data%2Finformation%20of%20the%20dataset [Accessed 09 November 2022].\n",
    "* Schork, J. (2019). *Mean Imputation for Missing Data (Example in R & SPSS)*, Statistics Globe [online], 24 September, availalbe: https://statisticsglobe.com/mean-imputation-for-missing-data/ [Accessed 09 Novmeber 2022].\n",
    "* Stack Overflow. (2012). *Hide all warnings in ipython*, Stack Overflow [online], 27 January, availalbe: https://stackoverflow.com/questions/9031783/hide-all-warnings-in-ipython [Accessed 09 November 2022].\n",
    "* Swalin, A. (2018). *How to Handle Missing Data*, Towards Data Science [online], 31 January, availalbe: https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4 [Accessed 09 November 2022]. \n",
    "* The Data Detective. (2019). *Preprocessing: Encode and KNN Impute All Categorical Features Fast*, Towards Data Science [online], 19 November, available: https://towardsdatascience.com/preprocessing-encode-and-knn-impute-all-categorical-features-fast-b05f50b4dfaa [Accessed 09 November 2022]. \n",
    "* Tirendaz Academy. (2022). *Penguin Dataset: Data Visualization with Seaborn*, Kaggle [online], 20 July, available: https://www.kaggle.com/code/tirendazacademy/penguin-dataset-data-visualization-with-seaborn [Accessed 09 November 2022]. \n",
    "* Tsai, E. (2021). *Build a Python project using Pandas and Seaborn*, Medium [online], 01 September, availalbe: https://medium.com/@marvelouskgc/build-a-penguin-project-using-pandas-and-seaborn-eugene-tsai-e4b7e0b499ea [Accessed 09 November 2022]. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Analytics Vidhya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  10 Bibliography\n",
    "------------------------------------------------------------------------------------------------\n",
    "* Geek for Geeks. (2022). *Check for NaN in Pandas DataFrame*, Geeks for Geeks [online], 26 August, availalbe: geeksforgeeks.org/check-for-nan-in-pandas-dataframe/ [Accessed 09 November 2022].\n",
    "* Gupta, D. (2018). *Pandas DataFrame: Playing with CSV files*, Medium [online], 02 November, availalbe: https://towardsdatascience.com/pandas-dataframe-playing-with-csv-files-944225d19ff [Accessed 08 November 2022]. \n",
    "* Ha, J. (2009). *How can I check for NaN values?*, Stack Overflow [online], 03 June, availalbe: https://stackoverflow.com/questions/944700/how-can-i-check-for-nan-values [Accessed 09 November 2022].\n",
    "* Horst, et al. (2022). *The R Journal: Palmer Archipelago Penguins Data in the palmerpenguins R Package - An Alternative to Anderson's Irises*, The R Journal, 1(1), pp. 7-16.\n",
    "* Rai, S. (2019). *3 Methods to Handle Missing Data*, Oracle [online], 28 Februry, available: https://blogs.oracle.com/ai-and-datascience/post/3-methods-to-handle-missing-data [Accessed 09 November 2022].\n",
    "* Stobierski, T. (2021). *DATA WRANGLING: WHAT IT IS & WHY IT’S IMPORTANT*, Harvard Business School [online], 19 January, availalbe: https://online.hbs.edu/blog/post/data-wrangling [Accessed 09 November 2022]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
